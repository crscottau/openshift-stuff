# Hosted Cluster Deployment

The creation of the Hosted Cluster leverages the Hosted Control Planes API resources, HostedCluster and NodePool. For this operation, we will be using the Web Console. You can find the required yaml files to run this same deployment here (you must change the pull secret content if you want to use it).

## Creating a Hosted Cluster

Access the Management Cluster OpenShift Console (remember you can use the bookmark) and log in with the OpenShift admin credentials.

On the top bar, next to the Red Hat OpenShift logo, make sure All Clusters is selected. This will show us the MultiCloud console.

Once you’re in, click on Infrastructure → Clusters. You will see a screen like the one below.

Click on Create cluster → Host inventory → Hosted.

You will get to a wizard that will guide you through the creation of the Hosted Cluster. Make sure you enter the following details.

IMPORTANT: You must use OpenShift Version 4.18.4 even if you see a newer one.

- Infrastructure provider credential: Leave it empty
- Cluster name: hosted
- Cluster set: Leave it empty
- Base domain: hypershift.lab
- OpenShift Version: OpenShift 4.18.4
- Pull secret: Put the output given by the command below:

```bash
oc --kubeconfig ~/hypershift-lab/mgmt-kubeconfig -n openshift-config \
    extract secret/pull-secret --to=- 2>/dev/null
```

In the next screen enter the following details.

- Controller availability policy: Single Replica
- Infrastructure availability policy: Single Replica
- OLM catalog placement: Management
- Namespace: hardware-inventory
- Use autoscaling: Unchecked
- Number of hosts: 2

The following screen will show the networking settings, these are the settings you should use.

- API server publishing strategy: LoadBalancer
- Use advanced networking: Unchecked
- Show proxy settings: Unchecked
- SSH public key: Put the output given by the command below:

```bash
oc --kubeconfig ~/hypershift-lab/mgmt-kubeconfig -n hardware-inventory \
    get infraenv/hosted -o jsonpath='{.spec.sshAuthorizedKey}'
```

At this point you can go ahead and click Create.

After clicking you will be redirected to the hosted cluster view.

## Monitoring the Hosted Cluster Deployment

In this section we will learn how we can monitor the Hosted Cluster deployment from the Web Console as well as from the CLI.

### Monitoring the Hosted Cluster Deployment via the Web Console

You should be in the hosted cluster view already from the previous step. If you’re not you can get back to it by opening the OpenShift Console. Making sure All Clusters is selected and clicking on the cluster named hosted.

You should see a screen where we see that most of the control plane components are ready.

IMPORTANT: It can take up to five minutes for the conditions to settle.

In this same view, we can click on the NodePool to see the nodes deployment status. We can see at this point two bare metal hosts from the inventory (hosted-worker0 and hosted-worker2) being installed.

IMPORTANT: The bare metal hosts selected may be different for your cluster.

After a few moments (around ten minutes), we will see that the nodes have been installed.

Additionally, we can click on Nodes in the hosted cluster view and we will see that the two nodes joined the Hosted Cluster.

At this point the Hosted Cluster is almost installed. In the next section we will see how to monitor the deployment from the CLI.

### Monitoring the Hosted Cluster Deployment via the CLI

Check the HostedCluster CR for the hosted cluster, you can see it says the hosted control plane is ready but the progress is Partial. This is expected since the Hosted Cluster deployment is not finished yet.

```bash
oc --kubeconfig ~/hypershift-lab/mgmt-kubeconfig -n hosted get \
    hostedcluster hosted
NAME     VERSION   KUBECONFIG                PROGRESS   AVAILABLE   PROGRESSING   MESSAGE
hosted             hosted-admin-kubeconfig   Partial    True        False         The hosted control plane is available
```

We can check the control plane pods.

```bash
oc --kubeconfig ~/hypershift-lab/mgmt-kubeconfig -n hosted-hosted get pods
NAME                                                  READY   STATUS    RESTARTS      AGE
capi-provider-5f67665fc-s92cf                         1/1     Running   0             21m
catalog-operator-7b8589ccff-wvsff                     2/2     Running   2 (18m ago)   18m
certified-operators-catalog-96c5fd665-ctkx2           1/1     Running   0             18m
cluster-api-6949888596-7v2fg                          1/1     Running   0             21m
cluster-autoscaler-57fd57cd5-mqwdx                    1/1     Running   0             18m
cluster-image-registry-operator-866c55fb67-klr56      2/2     Running   0             18m
cluster-network-operator-85d5b5c9cf-kf8ht             2/2     Running   0             18m
cluster-node-tuning-operator-cc669ffc4-9cq85          1/1     Running   0             18m
cluster-policy-controller-7479fb5685-tvwdz            1/1     Running   0             18m
cluster-storage-operator-59fb6b99fb-pn95w             1/1     Running   0             18m
cluster-version-operator-7b747f4d8d-x9fp4             1/1     Running   0             18m
community-operators-catalog-74b7c8d66d-qg2fg          1/1     Running   0             18m
control-plane-operator-7489787949-zk4jk               1/1     Running   0             21m
control-plane-pki-operator-b545ddf55-dtj9w            1/1     Running   0             21m
csi-snapshot-controller-5886f8d4bc-5dbr6              1/1     Running   0             18m
csi-snapshot-controller-operator-d8b554675-fdl8d      1/1     Running   0             18m
csi-snapshot-webhook-7cb9d884-9s5ms                   1/1     Running   0             18m
dns-operator-58866f6b5-78d86                          1/1     Running   0             18m
etcd-0                                                3/3     Running   0             21m
hosted-cluster-config-operator-9f8db7c98-zlv8h        1/1     Running   0             18m
ignition-server-7f58bcdcf8-4mr4c                      1/1     Running   0             18m
ignition-server-proxy-7494b7d785-7zvvg                1/1     Running   0             18m
ingress-operator-d5f4c6d6-nhqb9                       2/2     Running   0             18m
konnectivity-agent-6c5485d9bb-vwln7                   1/1     Running   0             18m
kube-apiserver-7c5bb9c695-vtt2d                       4/4     Running   0             20m
kube-controller-manager-544dd8d4f9-k2fr7              1/1     Running   0             11m
kube-scheduler-56477d87bd-qrfq2                       1/1     Running   0             19m
machine-approver-7b85988895-7cvc6                     1/1     Running   0             18m
multus-admission-controller-74ccdf8dc4-486f2          2/2     Running   0             12m
network-node-identity-546c9c477-rtqcd                 3/3     Running   0             12m
oauth-openshift-69dd979f7c-nh96v                      2/2     Running   0             18m
olm-operator-774bf68cb4-7w9k7                         2/2     Running   0             18m
openshift-apiserver-6b848587cf-dxvrv                  3/3     Running   0             11m
openshift-controller-manager-74c45dbcb7-p2gr7         1/1     Running   0             18m
openshift-oauth-apiserver-758dfd9bb4-qr565            2/2     Running   0             18m
openshift-route-controller-manager-576d8b7cfd-rldnv   1/1     Running   0             18m
ovnkube-control-plane-bfc6bb4fd-tl8rd                 3/3     Running   0             12m
packageserver-7f97b955c6-slctl                        2/2     Running   0             18m
redhat-marketplace-catalog-5fcdbb6578-gsljc           1/1     Running   0             18m
redhat-operators-catalog-5b5b659b64-8cknx             1/1     Running   0             18m
```

The NodePool will tell us the state of the nodes joining the Hosted Cluster:

```bash
oc --kubeconfig ~/hypershift-lab/mgmt-kubeconfig -n hosted get \
    nodepool nodepool-hosted-1
```

At this point the nodes are still being deployed as you can see.

```text
NAME                CLUSTER   DESIRED NODES   CURRENT NODES   AUTOSCALING   AUTOREPAIR   VERSION   UPDATINGVERSION   UPDATINGCONFIG   MESSAGE
nodepool-hosted-1   hosted    2               0               False         False        4.18.4                                       Scaling up MachineSet to 2 replicas (actual 0)
```

Since the nodes are being deployed we can check the status of the agents:

`oc --kubeconfig ~/hypershift-lab/mgmt-kubeconfig -n hardware-inventory get agents`

We can see at this point two agents from the inventory being rebooted as part of the installation process.

```text
NAME                                   CLUSTER   APPROVED   ROLE          STAGE
aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaa0201             true       auto-assign
aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaa0202   hosted    true       worker        Rebooting
aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaa0203   hosted    true       worker        Rebooting
```

After a few moments (up to ten minutes), the agents will be fully installed:

```bash
oc --kubeconfig ~/hypershift-lab/mgmt-kubeconfig -n hardware-inventory get agents
NAME                                   CLUSTER   APPROVED   ROLE          STAGE
aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaa0201             true       auto-assign
aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaa0202   hosted    true       worker        Done
aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaa0203   hosted    true       worker        Done
```

The NodePool will reflect that as well.

```bash
oc --kubeconfig ~/hypershift-lab/mgmt-kubeconfig -n hosted get \
    nodepool nodepool-hosted-1
NAME                CLUSTER   DESIRED NODES   CURRENT NODES   AUTOSCALING   AUTOREPAIR   VERSION   UPDATINGVERSION   UPDATINGCONFIG   MESSAGE
nodepool-hosted-1   hosted    2               2               False         False        4.18.4    False             False
```

At this point the Hosted Cluster deployment is not finished yet, since we need to fix Ingress for the cluster to be fully deployed. We will do that in the next section where we will learn how to access the Hosted Cluster.
